<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>9.5 Implementing the classifier - Data Science for Everyone</title>
<meta name="description" content="Implementing the Classifier">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_UK">
<meta property="og:site_name" content="Data Science for Everyone">
<meta property="og:title" content="9.5 Implementing the classifier">
<meta property="og:url" content="https://matthew-brett.github.io/dsfe/chapters/09/Implementing_the_Classifier">


  <meta property="og:description" content="Implementing the Classifier">







  <meta property="article:published_time" content="2019-03-29T12:04:31+00:00">





  

  


<link rel="canonical" href="https://matthew-brett.github.io/dsfe/chapters/09/Implementing_the_Classifier">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Matthew Brett",
      "url": "https://matthew-brett.github.io/dsfe",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/dsfe/feed.xml" type="application/atom+xml" rel="alternate" title="Data Science for Everyone Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/dsfe/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->


<!-- end custom head snippets -->

    <link rel="stylesheet" href="/dsfe/assets/css/notebook-markdown.css">
    <link rel="stylesheet" href="/dsfe/assets/css/custom.css">
    <link rel="shortcut icon" type="image/png" href="/dsfe/favicon.png">
    <script src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js"></script>
  </head>

  <body class="layout--textbook">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

    <div class="initial-content">
      



<div id="main" class="textbook" role="main">
  <div id="textbook_wrapper">
    
  <div class="sidebar sticky textbook">
  
  
    <img src="/dsfe/images/dsfe_logo.png" class="textbook_logo" />
    

    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          
          

          <a href="/dsfe/"><span class="nav__sub-title">Home</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/01/what-is-data-science"><span class="nav__sub-title">1. Data Science</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/intro" class="level_1">1.1 Introduction</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/computational-tools" class="level_2">1.1.1 Computational Tools</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/statistical-techniques" class="level_2">1.1.2 Statistical Techniques</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/why-data-science" class="level_1">1.2 Why Data Science?</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/Plotting_the_Classics" class="level_1">1.3 Plotting the Classics</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/Literary_Characters" class="level_2">1.3.1 Literary Characters</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/01/Another_Kind_Of_Character" class="level_2">1.3.2 Another Kind of Character</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/02/to_code"><span class="nav__sub-title">2. Programming</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/sampling_problem" class="level_1">2.1 A sampling problem</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/three_girls" class="level_1">2.2 A simpler problem</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/Expressions" class="level_1">2.3 Expressions</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/variables" class="level_1">2.4 Variables</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/Names" class="level_1">2.5 Names</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/02/Calls" class="level_1">2.6 Call expressions</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/03/data_types"><span class="nav__sub-title">3. Data types</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/Numbers" class="level_1">3.1 Numbers</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/Strings" class="level_1">3.2 Strings</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/Comparison" class="level_1">3.3 Comparison</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/Arrays" class="level_1">3.4 Arrays</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/Ranges" class="level_1">3.5 Ranges</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/More_on_Arrays" class="level_1">3.6 More on arrays</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/arrays_and_axes" class="level_1">3.7 Arrays and axes</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/reply_supreme" class="level_1">3.8 Reply to the Supreme Court</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/three_girls" class="level_1">3.9 Revision - three girls</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/03/array_indexing" class="level_1">3.10 Selecting with arrays</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/04/data_frames"><span class="nav__sub-title">4. Data frames</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/04/data_frame_intro" class="level_1">4.1 Introduction to data frames</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/05/permutation"><span class="nav__sub-title">5. Permutations</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/population_permutation" class="level_1">5.1 Population and permutation</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/lists" class="level_1">5.2 lists</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/iteration" class="level_1">5.3 Iteration with For loops</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/indentation" class="level_1">5.4 Indentation, indentation</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/ones_zeros" class="level_1">5.5 Ones and zeros</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/05/brexit_ages" class="level_1">5.6 A permutation test</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/06/more_simulation"><span class="nav__sub-title">6. More on simulation</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/06/sorting_arrays" class="level_1">6.1 Sorting arrays</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/06/monty_hall" class="level_1">6.2 Monty hall problem</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/07/more_building_blocks"><span class="nav__sub-title">7. More building blocks</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/07/none" class="level_1">7.2 None</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/07/functions" class="level_1">7.1 Functions</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/07/functions_as_values" class="level_1">7.1 Functions as values</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/07/conditional_statements" class="level_1">7.2 Conditional statements</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/08/mean"><span class="nav__sub-title">8. The mean and straight line relationships</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/mean_meaning" class="level_1">8.1 The mean as a predictor</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/where_and_argmin" class="level_1">8.2 Where and argmin</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/mean_and_slopes" class="level_1">8.3 Mean and slopes</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/optimization" class="level_1">8.4 Optimization</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/finding_lines" class="level_1">8.5 Finding lines</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/08/inference_on_slopes" class="level_1">8.6 Believable slopes</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/09/classification"><span class="nav__sub-title">9. Classification</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/standard_scores" class="level_1">9.1 Standard scores</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/Nearest_Neighbors" class="level_1">9.2 Nearest neighbors</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/Training_and_Testing" class="level_1">9.3 Training and testing</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/Rows_of_Tables" class="level_1">9.4 Rows of tables</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/Implementing_the_Classifier" class="level_1">9.5 Implementing the classifier</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/09/Accuracy_of_the_Classifier" class="level_1">9.6 Accuracy of the classifier</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/exercises/exercises"><span class="nav__sub-title">Exercises</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/simulation" class="level_1">Simulation</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/df_exercises" class="level_1">Data frames</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/brexit_analysis" class="level_1">Brexit analysis</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/for_loops" class="level_1">For loops</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/money_and_death" class="level_1">Money and death</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/function_exercises" class="level_1">Function exercises</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/functions_values_exercises" class="level_1">Function as values exercises</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/exercises/conditional_statements_exercises" class="level_1">Conditional statement exercises</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/dsfe/chapters/extra/extra"><span class="nav__sub-title">Extra pages</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/extra/more_on_lists" class="level_1">More on lists</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/extra/monty_hall_lists" class="level_1">Monty Hall with lists</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/extra/data8_functions" class="level_1">Berkeley introduction to functions</a></li>
          
            
            

            
            

            

            <li><a href="/dsfe/chapters/extra/mean_deviations" class="level_1">Deviations around the mean</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    

  
  </div>


    <article class="page textbook" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="headline" content="9.5 Implementing the classifier">
      <meta itemprop="description" content="Implementing the Classifier">
      <meta itemprop="datePublished" content="March 29, 2019">
      

      <div class="page__inner-wrap">
        
          <header>
            <h1 id="page-title" class="page__title" itemprop="headline">9.5 Implementing the classifier
</h1>
          </header>
        

        <section class="page__content" itemprop="text">
          
            

<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-list-ul"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#implementing-the-classifier">Implementing the Classifier</a></li>
  <li><a href="#banknote-authentication">Banknote authentication</a></li>
  <li><a href="#multiple-attributes">Multiple attributes</a></li>
  <li><a href="#distance-in-multiple-dimensions">Distance in Multiple Dimensions</a></li>
  <li><a href="#a-plan-for-the-implementation">A Plan for the Implementation</a></li>
  <li><a href="#implementation-step-1">Implementation Step 1</a></li>
  <li><a href="#implementation-steps-2-and-3">Implementation Steps 2 and 3</a></li>
</ul>
    </nav>
  </aside>


          
          <!-- INTERACT LINKS -->

    
    
    <a class="notebook-link" href="https://matthew-brett.github.io/dsfe/notebooks/09/Implementing_the_Classifier.ipynb">Download notebook</a>
    <a class="interact-button" href="https://mybinder.org/v2/gh/matthew-brett/dsfe/master?filepath=notebooks%2F09%2FImplementing_the_Classifier.ipynb">Interact</a>


          <h3 id="implementing-the-classifier">Implementing the Classifier</h3>

<p>We are now ready to implement a $k$-nearest neighbor classifier based on multiple attributes. We have used only two attributes so far, for ease of visualization. But usually predictions will be based on many attributes. Here is an example that shows how multiple attributes can be better than pairs.</p>

<h3 id="banknote-authentication">Banknote authentication</h3>

<p>This time we’ll look at predicting whether a banknote (e.g., a \$20 bill) is counterfeit or legitimate.  Researchers have put together a data set for us, based on photographs of many individual banknotes: some counterfeit, some legitimate.  They computed a few numbers from each image, using techniques that we won’t worry about for this course.  So, for each banknote, we know a few numbers that were computed from a photograph of it as well as its class (whether it is counterfeit or not).  Let’s load it into a table and take a look.</p>

<p>If you are running on your laptop, you should download the
<a href="/dsfe/data/banknotes.csv">banknotes</a> file to the
same directory as this notebook.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">banknotes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'banknote.csv'</span><span class="p">)</span>
<span class="n">banknotes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WaveletVar</th>
      <th>WaveletSkew</th>
      <th>WaveletCurt</th>
      <th>Entropy</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.62160</td>
      <td>8.6661</td>
      <td>-2.8073</td>
      <td>-0.44699</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.54590</td>
      <td>8.1674</td>
      <td>-2.4586</td>
      <td>-1.46210</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.86600</td>
      <td>-2.6383</td>
      <td>1.9242</td>
      <td>0.10645</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.45660</td>
      <td>9.5228</td>
      <td>-4.0112</td>
      <td>-3.59440</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.32924</td>
      <td>-4.4552</td>
      <td>4.5718</td>
      <td>-0.98880</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<p>Let’s look at whether the first two numbers tell us anything about whether the banknote is counterfeit or not.  Here’s a scatterplot:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">banknotes</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'darkblue'</span>
<span class="n">banknotes</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">banknotes</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'Color'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'gold'</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">banknotes</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'WaveletVar'</span><span class="p">,</span>
                       <span class="s">'WaveletCurt'</span><span class="p">,</span>
                       <span class="n">c</span><span class="o">=</span><span class="n">banknotes</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_5_0.png" alt="png" /></p>

<p>Pretty interesting!  Those two measurements do seem helpful for predicting whether the banknote is counterfeit or not.  However, in this example you can now see that there is some overlap between the blue cluster and the gold cluster.  This indicates that there will be some images where it’s hard to tell whether the banknote is legitimate based on just these two numbers.  Still, you could use a $k$-nearest neighbor classifier to predict the legitimacy of a banknote.</p>

<p>Take a minute and think it through: Suppose we used $k=11$ (say).  What parts of the plot would the classifier get right, and what parts would it make errors on?  What would the decision boundary look like?</p>

<p>The patterns that show up in the data can get pretty wild.  For instance, here’s what we’d get if used a different pair of measurements from the images:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">banknotes</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'WaveletSkew'</span><span class="p">,</span> <span class="s">'Entropy'</span><span class="p">,</span>
                       <span class="n">c</span><span class="o">=</span><span class="n">banknotes</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_7_0.png" alt="png" /></p>

<p>There does seem to be a pattern, but it’s a pretty complex one.  Nonetheless, the $k$-nearest neighbors classifier can still be used and will effectively “discover” patterns out of this.  This illustrates how powerful machine learning can be: it can effectively take advantage of even patterns that we would not have anticipated, or that we would have thought to “program into” the computer.</p>

<h3 id="multiple-attributes">Multiple attributes</h3>

<p>So far I’ve been assuming that we have exactly 2 attributes that we can use to help us make our prediction.  What if we have more than 2?  For instance, what if we have 3 attributes?</p>

<p>Here’s the cool part: you can use the same ideas for this case, too.  All you have to do is make a 3-dimensional scatterplot, instead of a 2-dimensional plot.  You can still use the $k$-nearest neighbors classifier, but now computing distances in 3 dimensions instead of just 2.  It just works.  Very cool!</p>

<p>In fact, there’s nothing special about 2 or 3.  If you have 4 attributes, you can use the $k$-nearest neighbors classifier in 4 dimensions.  5 attributes?  Work in 5-dimensional space.  And no need to stop there!  This all works for arbitrarily many attributes; you just work in a very high dimensional space.  It gets wicked-impossible to visualize, but that’s OK.  The computer algorithm generalizes very nicely: all you need is the ability to compute the distance, and that’s not hard.  Mind-blowing stuff!</p>

<p>For instance, let’s see what happens if we try to predict whether a banknote is counterfeit or not using 3 of the measurements, instead of just 2.  Here’s what you get:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">banknotes</span><span class="p">[</span><span class="s">'WaveletSkew'</span><span class="p">],</span>
           <span class="n">banknotes</span><span class="p">[</span><span class="s">'WaveletVar'</span><span class="p">],</span>
           <span class="n">banknotes</span><span class="p">[</span><span class="s">'WaveletCurt'</span><span class="p">],</span>
           <span class="n">c</span><span class="o">=</span><span class="n">banknotes</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_10_0.png" alt="png" /></p>

<p>Awesome!  With just 2 attributes, there was some overlap between the two clusters (which means that the classifier was bound to make some mistakes for pointers in the overlap).  But when we use these 3 attributes, the two clusters have almost no overlap.  In other words, a classifier that uses these 3 attributes will be more accurate than one that only uses the 2 attributes.</p>

<p>This is a general phenomenon in classification.  Each attribute can potentially give you new information, so more attributes sometimes helps you build a better classifier.  Of course, the cost is that now we have to gather more information to measure the value of each attribute, but this cost may be well worth it if it significantly improves the accuracy of our classifier.</p>

<p>To sum up: you now know how to use $k$-nearest neighbor classification to predict the answer to a yes/no question, based on the values of some attributes, assuming you have a training set with examples where the correct prediction is known.  The general roadmap is this:</p>

<ol>
  <li>identify some attributes that you think might help you predict the answer to the question.</li>
  <li>Gather a training set of examples where you know the values of the attributes as well as the correct prediction.</li>
  <li>To make predictions in the future, measure the value of the attributes and then use $k$-nearest neighbor classification to predict the answer to the question.</li>
</ol>

<h3 id="distance-in-multiple-dimensions">Distance in Multiple Dimensions</h3>

<p>We know how to compute distance in 2-dimensional space. If we have a point at coordinates $(x_0,y_0)$ and another at $(x_1,y_1)$, the distance between them is</p>

<script type="math/tex; mode=display">D = \sqrt{(x_0-x_1)^2 + (y_0-y_1)^2}.</script>

<p>In 3-dimensional space, the points are $(x_0, y_0, z_0)$ and $(x_1, y_1, z_1)$, and the formula for the distance between them is</p>

<script type="math/tex; mode=display">D = \sqrt{(x_0-x_1)^2 + (y_0-y_1)^2 + (z_0-z_1)^2}</script>

<p>In $n$-dimensional space, things are a bit harder to visualize, but I think you can see how the formula generalized: we sum up the squares of the differences between each individual coordinate, and then take the square root of that.</p>

<p>In the last section, we defined the function <code class="highlighter-rouge">distance</code> which returned the distance between two points. We used it in two-dimensions, but the great news is that the function doesn’t care how many dimensions there are! It just subtracts the two arrays of coordinates (no matter how long the arrays are), squares the differences and adds up, and then takes the square root. To work in multiple dimensions, we don’t have to change the code at all.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>
    <span class="s">"""Returns the distance between point1 and point2
    where each argument is an array 
    consisting of the coordinates of the point"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">point1</span> <span class="o">-</span> <span class="n">point2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div></div>

<p>Let’s use this on a <a href="https://archive.ics.uci.edu/ml/datasets/Wine">new dataset</a>. The table <code class="highlighter-rouge">wine</code> contains the chemical composition of 178 different Italian wines. The classes are the grape species, called cultivars. There are three classes but let’s just see whether we can tell Class 1 apart from the other two.</p>

<p>You can download the file from
<a href="/dsfe/data/wine.csv">wine.csv</a>.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'wine.csv'</span><span class="p">)</span>
<span class="n">wine</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class</th>
      <th>Alcohol</th>
      <th>Malic Acid</th>
      <th>Ash</th>
      <th>Alcalinity of Ash</th>
      <th>Magnesium</th>
      <th>Total Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color Intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diulted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<p>The dataset classifies the wine into three classes:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2    71
1    59
3    48
Name: Class, dtype: int64
</code></pre></div></div>

<p>For the moment, we’ll pool classes 2 and 3 into one class, <code class="highlighter-rouge">0</code>:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">wine</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'Class'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">wine</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class</th>
      <th>Alcohol</th>
      <th>Malic Acid</th>
      <th>Ash</th>
      <th>Alcalinity of Ash</th>
      <th>Magnesium</th>
      <th>Total Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color Intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diulted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<p>The first two wines are both in Class 1. To find the distance between them, we first need a table of just the attributes:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine_attributes</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'Class'</span><span class="p">)</span>
<span class="n">wine_attributes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Alcohol</th>
      <th>Malic Acid</th>
      <th>Ash</th>
      <th>Alcalinity of Ash</th>
      <th>Magnesium</th>
      <th>Total Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color Intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diulted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
         <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>31.265012394048398
</code></pre></div></div>

<p>The last wine in the table is of Class 0. Its distance from the first wine is:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
         <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">177</span><span class="p">]))</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>506.05936766351834
</code></pre></div></div>

<p>That’s quite a bit bigger! Let’s do some visualization to see if Class 1 really looks different from Class 0.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine_with_colors</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">wine_with_colors</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'darkblue'</span>
<span class="n">wine_with_colors</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">wine</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'Color'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'gold'</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'Flavanoids'</span><span class="p">,</span> <span class="s">'Alcohol'</span><span class="p">,</span>
                              <span class="n">c</span><span class="o">=</span><span class="n">wine_with_colors</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_28_0.png" alt="png" /></p>

<p>The blue points (Class 1) are almost entirely separate from the gold ones. That is one indication of why the distance between two Class 1 wines would be smaller than the distance between wines of two different classes. We can see a similar phenomenon with a different pair of attributes too:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'Alcalinity of Ash'</span><span class="p">,</span> <span class="s">'Ash'</span><span class="p">,</span>
                              <span class="n">c</span><span class="o">=</span><span class="n">wine_with_colors</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_30_0.png" alt="png" /></p>

<p>But for some pairs the picture is more murky.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'Magnesium'</span><span class="p">,</span> <span class="s">'Total Phenols'</span><span class="p">,</span>
                              <span class="n">c</span><span class="o">=</span><span class="n">wine_with_colors</span><span class="p">[</span><span class="s">'Color'</span><span class="p">]);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/09/Implementing_the_Classifier_32_0.png" alt="png" /></p>

<p>Let’s see if we can implement a classifier based on all of the attributes. After that, we’ll see how accurate it is.</p>

<h3 id="a-plan-for-the-implementation">A Plan for the Implementation</h3>

<p>It’s time to write some code to implement the classifier.  The input is a <code class="highlighter-rouge">point</code> that we want to classify.  The classifier works by finding the $k$ nearest neighbors of <code class="highlighter-rouge">point</code> from the training set.  So, our approach will go like this:</p>

<ol>
  <li>
    <p>Find the closest $k$ neighbors of <code class="highlighter-rouge">point</code>, i.e., the $k$ wines from the training set that are most similar to <code class="highlighter-rouge">point</code>.</p>
  </li>
  <li>
    <p>Look at the classes of those $k$ neighbors, and take the majority vote to find the most-common class of wine.  Use that as our predicted class for <code class="highlighter-rouge">point</code>.</p>
  </li>
</ol>

<p>So that will guide the structure of our Python code.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="s">""" Find the closest k neighbors of p """</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="s">""" Return majority vote from top k classes """</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">kclosest</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">kclosest</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">kclosest</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">kclosest</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="implementation-step-1">Implementation Step 1</h3>

<p>To implement the first step for the kidney disease data, we had to compute the distance from each patient in the training set to <code class="highlighter-rouge">point</code>, sort them by distance, and take the $k$ closest patients in the training set.</p>

<p>That’s what we did in the previous section with the point corresponding to Alice. Let’s generalize that code. We’ll redefine <code class="highlighter-rouge">distance</code> here, just for convenience.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>
    <span class="s">"""Returns the distance between point1 and point2
    where each argument is an array
    consisting of the coordinates of the point"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">point1</span> <span class="o">-</span> <span class="n">point2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">all_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">):</span>
    <span class="s">"""Returns an array of distances
    between each point in the training set
    and the new point (which is a row of attributes)"""</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'Class'</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">distance_from_point</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_point</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">row</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">attributes</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">distance_from_point</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">table_with_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">):</span>
    <span class="s">"""Augments the training table
    with a column of distances from new_point"""</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">out</span><span class="p">[</span><span class="s">'Distance'</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="s">"""Returns a table of the k rows of the augmented table
    corresponding to the k smallest distances"""</span>
    <span class="n">with_dists</span> <span class="o">=</span> <span class="n">table_with_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">)</span>
    <span class="n">sorted_by_distance</span> <span class="o">=</span> <span class="n">with_dists</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'Distance'</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sorted_by_distance</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">topk</span>
</code></pre></div></div>

<p>Let’s see how this works on our <code class="highlighter-rouge">wine</code> data. We’ll just take the first wine and find its five nearest neighbors among all the wines. Remember that since this wine is part of the dataset, it is its own nearest neighbor. So we should expect to see it at the top of the list, followed by four others.</p>

<p>First let’s extract its attributes:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">special_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>And now let’s find its 5 nearest neighbors.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">closest</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class</th>
      <th>Alcohol</th>
      <th>Malic Acid</th>
      <th>Ash</th>
      <th>Alcalinity of Ash</th>
      <th>Magnesium</th>
      <th>Total Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color Intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diulted wines</th>
      <th>Proline</th>
      <th>Distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>54</th>
      <td>1</td>
      <td>13.74</td>
      <td>1.67</td>
      <td>2.25</td>
      <td>16.4</td>
      <td>118</td>
      <td>2.60</td>
      <td>2.90</td>
      <td>0.21</td>
      <td>1.62</td>
      <td>5.85</td>
      <td>0.92</td>
      <td>3.20</td>
      <td>1060</td>
      <td>10.392805</td>
    </tr>
    <tr>
      <th>45</th>
      <td>1</td>
      <td>14.21</td>
      <td>4.04</td>
      <td>2.44</td>
      <td>18.9</td>
      <td>111</td>
      <td>2.85</td>
      <td>2.65</td>
      <td>0.30</td>
      <td>1.25</td>
      <td>5.24</td>
      <td>0.87</td>
      <td>3.33</td>
      <td>1080</td>
      <td>22.340748</td>
    </tr>
    <tr>
      <th>48</th>
      <td>1</td>
      <td>14.10</td>
      <td>2.02</td>
      <td>2.40</td>
      <td>18.8</td>
      <td>103</td>
      <td>2.75</td>
      <td>2.92</td>
      <td>0.32</td>
      <td>2.38</td>
      <td>6.20</td>
      <td>1.07</td>
      <td>2.75</td>
      <td>1060</td>
      <td>24.760232</td>
    </tr>
    <tr>
      <th>46</th>
      <td>1</td>
      <td>14.38</td>
      <td>3.59</td>
      <td>2.28</td>
      <td>16.0</td>
      <td>102</td>
      <td>3.25</td>
      <td>3.17</td>
      <td>0.27</td>
      <td>2.19</td>
      <td>4.90</td>
      <td>1.04</td>
      <td>3.44</td>
      <td>1065</td>
      <td>25.094663</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<p>Bingo! The first row is the nearest neighbor, which is itself – there’s a 0 in the <code class="highlighter-rouge">Distance</code> column as expected. All five nearest neighbors are of Class 1, which is consistent with our earlier observation that Class 1 wines appear to be clumped together in some dimensions.</p>

<h3 id="implementation-steps-2-and-3">Implementation Steps 2 and 3</h3>

<p>Next we need to take a “majority vote” of the nearest neighbors and assign our point the same class as the majority.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">topkclasses</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">topkclasses</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ones</span> <span class="o">&gt;</span> <span class="n">zeros</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">closestk</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classify</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1
</code></pre></div></div>

<p>If we change <code class="highlighter-rouge">special_wine</code> to be the last one in the dataset, is our classifier able to tell that it’s in Class 0?</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">special_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">177</span><span class="p">]</span>
<span class="n">classify</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0
</code></pre></div></div>

<p>Yes! The classifier gets this one right too.</p>

<p>But we don’t yet know how it does with all the other wines, and in any case we know that testing on wines that are already part of the training set might be over-optimistic. In the final section of this chapter, we will separate the wines into a training and test set and then measure the accuracy of our classifier on the test set.</p>

<div class="isa_info">
<i class="fa fa-info-circle"></i>
This page has content from the
<a href="https://github.com/data-8/textbook/blob/64b20f0/notebooks/Implementing_the_Classifier.ipynb">
Implementing_the_Classifier</a>
notebook from the UC Berkeley course.  See the Berkeley course section of the
<a href="/dsfe/license">license</a>
</div>


          
        </section>

        <footer class="page__meta">
          
          


        </footer>

        

        
  <nav class="pagination">
    
      <a href="/dsfe/chapters/09/Rows_of_Tables" class="pagination--pager" title="9.4 Rows of tables
">Previous</a>
    
    
      <a href="/dsfe/chapters/09/Accuracy_of_the_Classifier" class="pagination--pager" title="9.6 Accuracy of the classifier
">Next</a>
    
  </nav>


      </div>

      
    </article>
  </div>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    

    
  <script src="/dsfe/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>




<script src="/dsfe/assets/js/lunr/lunr.min.js"></script>
<script src="/dsfe/assets/js/lunr/lunr-store.js"></script>
<script src="/dsfe/assets/js/lunr/lunr-en.js"></script>




    <!-- Custom scripts to load after site JS is loaded -->

    <!-- Custom HTML used for the textbooks -->
<!-- Configure, then load MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      processEnvironments: true
    }
  };
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full,Safe" type="text/javascript"></script>


<script type="text/javascript">
// --- To auto-embed hub URLs in interact links if given in a RESTful fashion ---
function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return jQuery.param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = $("a").each(function() {
    var href = this.href;
    // If the link is an internal link...
    if (href.search("https://matthew-brett.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['hub'] = hub;
      } else {
        // Create the REST params
        params = {'hub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + jQuery.param(params);
      this.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}

  // Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    hubUrl = rest['hub'];
    if (hubUrl !== undefined) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);
      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      link = $("a.interact-button")[0];
      if (link !== undefined) {
          // Update the interact link URL
          var href = link.getAttribute('href');
          if ('binder' == 'binder') {
            // If binder links exist, we need to re-work them for jupyterhub
            first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
            href = first + '?' + binder2Jupyterhub(href);
          } else {
            // If JupyterHub links, we only need to replace the hub url
            href = href.replace("https://mybinder.org", hubUrl);
          }
          link.setAttribute('href', decodeURIComponent(href));

          // Add text after interact link saying where we're launching
          hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
          $("a.interact-button").after($('<div class="interact-context">on ' + hubUrlNoHttp + '</div>'));

      }
      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

// --- Highlight the part of sidebar for current page ---

// helper to replace trailing slash
function replaceSlash(string)
{
    return string.replace(/\/$/, "");
}

// Add a class to the current page in the sidebar
function highlightSidebarCurrentPage()
{
  var currentpage = location.href;
  var links = $('.sidebar .nav__items a');
  var ii = 0;
  for(ii; ii < links.length; ii++) {
    var link = links[ii];
    if(replaceSlash(link.href) == replaceSlash(currentpage)) {
      // Add CSS for styling
      link.classList.add("current");
      // Scroll to this element
      $('div.sidebar').scrollTop(link.offsetTop - 300);
    }
  }
}

// --- Set up copy/paste for code blocks ---
function addCopyButtonToCode(){
  // get all <code> elements
  var allCodeBlocksElements = $( "div.input_area code, div.highlighter-rouge code" );

  allCodeBlocksElements.each(function(ii) {
   	// add different id for each code block

  	// target
    var currentId = "codeblock" + (ii + 1);
    $(this).attr('id', currentId);

    //trigger
    var clipButton = '<button class="btn copybtn" data-clipboard-target="#' + currentId + '"><img src="https://clipboardjs.com/assets/images/clippy.svg" width="13" alt="Copy to clipboard"></button>';
       $(this).after(clipButton);
    });

    new Clipboard('.btn');
}

// Run scripts when page is loaded
$(document).ready(function () {
  // Add anchors to H1 etc links
  anchors.add();
  // Highlight current page in sidebar
  highlightSidebarCurrentPage();
  // Add copy button to code blocks
  addCopyButtonToCode();
  // Update the Interact link if a REST param given
  updateInteractLink();
});
</script>

  </body>
</html>
